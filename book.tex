\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsfonts}
\usepackage{listings}
\usepackage{fontspec}
\usepackage[bookmarks, hidelinks]{hyperref}
\usepackage{array} % For tables
\usepackage{graphicx} % 1. Include the graphicx package

% Set JetBrains Mono as the default monospaced font
\setmonofont{JetBrainsMono-Regular}

% Restore original IntelliJ-style code formatting
\lstdefinestyle{intelliJStyle}{
    language=Java,
    basicstyle=\ttfamily\small,
    numbers=left,
    numberstyle=\tiny,
    stepnumber=1,
    numbersep=5pt,
    frame=single,
    breaklines=true,
    breakatwhitespace=true,
    tabsize=1,
    showstringspaces=false,
    captionpos=b
}
\lstset{style=intelliJStyle}

\lstdefinelanguage{JavaScript}{
	keywords={
		break, case, catch, class, const, continue, debugger, default, delete, do, 
		else, export, extends, finally, for, function, if, import, in, instanceof, 
		new, return, super, switch, this, throw, try, typeof, var, void, while, with, 
		yield, static, async, await, enum, implements, interface, let, package, private, 
		protected, public, arguments
	},
	morestring=[b]",
	morestring=[b]',
	morestring=[b]\`, % Template literal support
	comment=[l]//,
	comment=[s]{/*}{*/},
	ndkeywords={
		Array, Boolean, Date, Error, EvalError, Function, Infinity, JSON, Math, NaN, 
		Number, Object, Promise, Proxy, Reflect, RegExp, Set, String, Symbol, 
		TypeError, URIError, WeakMap, WeakSet, undefined, null, console, window, 
		document, alert, setTimeout, setInterval, clearTimeout, clearInterval, 
		fetch, require, module, process
	},
	keywordstyle=\bfseries, % Keywords will be bold
	ndkeywordstyle=\slshape, % Non-declared keywords will be slanted/italic
	stringstyle=\ttfamily, % Strings will be monospaced (typewriter) font
	commentstyle=\itshape\small, % Comments will be italic and slightly smaller
	identifierstyle=, % Default style for identifiers (usually regular text)
	sensitive=true
}

% Restore original Maple language definition
\lstdefinelanguage{Maple}{
    sensitive=true,
    morecomment=[l]{--},
    morecomment=[s]{/*}{*/},
    morestring=[b]",
    morestring=[b]',
    morekeywords={and,assuming,do,else,end,export,finally,for,if,implies,in,local,module,next,not,option,or,proc,quit,read,return,save,then,use,while},
    morekeywords=[2]{array,begin,by,case,description,elif,except,fi,proc,od,otherwise,repeat,return,select,then,until,when,where},
    morekeywords=[3]{diff,int,factor,integrate,limit,signum,sum},
    alsoletter={\$},
    literate=
        {>}{{\textgreater}}1
        {<}{{\textless}}1
}

\usepackage{titlesec}
\newcommand{\sectionbreak}{\clearpage}

\usepackage{float}
\usepackage{tabularx}

\usepackage{booktabs}

\begin{document}
\bibliographystyle{plain}

\title{Fibonacci Numbers: A Deep Dive (beta)}
\author{Stanislav Ostapenko}
\date{\today}
\maketitle

\begin{abstract}
This article explores the Fibonacci sequence from basic implementations to advanced mathematical techniques. We begin with a naive recursive method, highlighting its exponential complexity and stack limitations in Java. Using a C++ JVMTI agent, we analyze JVM stack frames to understand StackOverflowException. We address Java's \texttt{long} type limitations with BigDecimal for large numbers. Optimization techniques like memoization and dynamic programming are introduced to improve performance. We derive Binet's formula using formal power series and explore matrix exponentiation for logarithmic-time computation. Finally, we discuss real-world applications, including the golden ratio, algorithms, and financial modeling.
\end{abstract}

\clearpage

	\tableofcontents % Generate table of contents

	\clearpage
	
	\lstlistoflistings % Generate list of listings

\clearpage % or \newpage

\clearpage

	\thispagestyle{empty}

	\vspace*{\fill}
	\begin{center}
		\Huge
		\begin{align*}
			&   \mathcal{O}(1) &&= \mathcal{O}(\text{yeah})\\
			&    \mathcal{O}(\log_{} n) &&= \mathcal{O}(\text{nice})\\
			&    \mathcal{O}(n) &&= \mathcal{O}(\text{k})\\
			&    \mathcal{O}(n^{2}) &&= \mathcal{O}(\text{my})\\
			&    \mathcal{O}(2^{n}) &&= \mathcal{O}(\text{no})\\
			&    \mathcal{O}(n!) &&= \mathcal{O}(\text{mg})\\
			&    \mathcal{O}(n^{n}) &&= \mathcal{O}(\text{sh*t!})
		\end{align*}
	\end{center}
	\vspace*{\fill}


\clearpage % or \newpage

\section{Recursion and Mathematical Induction}
The Fibonacci sequence, defined as $F_n = F_{n-1} + F_{n-2}$ with $F_0 = 0$ and $F_1 = 1$, is a fundamental concept in mathematics and computer science. Introduced by Leonardo of Pisa in 1202, it appears in nature (e.g., spiral patterns), algorithms (e.g., Fibonacci heaps), and number theory. We'll start our journey from naive recursion to advanced techniques, analyzing their computational complexity and practical limitations.

The concepts of recursion and mathematical induction are closely intertwined, as both rely on solving problems by breaking them down into smaller instances and establishing a base case. Below, we explore their relationship through their structural similarities and shared principles, with a particular emphasis on the role of the base case.

In mathematical induction, the base case establishes the truth of a statement for an initial value. In recursion, the base case is equally critical, as it defines the condition under which the recursive process terminates, returning a specific value without further recursive calls. The base case prevents infinite recursion and provides a foundation for building solutions to larger instances. Without a well-defined base case, a recursive function would continue indefinitely, leading to errors such as stack overflow.

For example, in a recursive factorial function, the base case is typically defined for \( n = 0 \) or \( n = 1 \), returning 1. This ensures that the recursion stops at a known value, allowing the algorithm to compute results for larger inputs by building on this foundation.

The base case is the cornerstone of both recursion and mathematical induction:
\begin{itemize}
	\item \textbf{Termination}: In recursion, the base case ensures the process stops, preventing infinite recursion. Without it, the function would attempt to compute values for invalid inputs (e.g., negative numbers) or never terminate.
	\item \textbf{Correctness}: The base case aligns with the mathematical definition of the problem, ensuring accurate results. For factorial, \( 0! = 1 \) and \( 1! = 1 \) are standard definitions.
	\item \textbf{Foundation}: It provides a starting point that recursive calls or inductive steps rely on to build the solution or proof.
\end{itemize}

Both recursion and mathematical induction rely on the principle of breaking down a problem into simpler components:
\begin{itemize}
	\item \textbf{Mathematical induction} proves a statement for all cases by starting with a base case and using the inductive step to cover all subsequent cases.
	\item \textbf{Recursion} computes a result by solving smaller instances of the same problem, reducing it to the base case.
\end{itemize}

Recursion and mathematical induction share a fundamental approach: solving or proving something by reducing it to simpler cases, anchored by a well-defined base case. The base case is essential for termination, correctness, and providing a foundation for building solutions or proofs. While induction is a proof technique, recursion is its practical counterpart in programming, with the base case playing a pivotal role in ensuring both processes succeed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Naive Recursion}
\subsection{Algorithm in Java}
The simplest approach to compute Fibonacci numbers is recursion, following the sequence's definition. We have base case conditions for \( F(0) \) and \( F(1) \), and for \( n \geq 2 \), we recursively compute \( F(n-1) + F(n-2) \).

\lstinputlisting[language=Java,caption={Naive Recursive Fibonacci in Java}]{./optimizing-recursion/NaiveRecursion.java}

Also we introduce common interface for Fibonacci algorithms to compare them later.
\lstinputlisting[language=Java,caption={Fibonacci interface}, nolol]{./optimizing-recursion/FibonacciSequence.java}

This method is intuitive but inefficient due to redundant calculations, forming a binary recursion tree with approximately $2^n$ nodes. Which comes from the fact that each call to \( F(n) \) results in two further calls for \( F(n-1) \) and \( F(n-2) \).
It's simmilar to binary tree where each node has two children. The depth of this tree is \( n \), and the number of nodes grows exponentially with \( n \).
Later, we will explore optimizations to improve performance. Now, let's analyze the time complexity of this naive recursive approach.
We start from the derivation of Binet's formula, which provides a closed-form expression for Fibonacci numbers in a form of $F(n)$ without elements of previous sequence.

\subsection{Binet's formula}
\subsubsection{Intuitive Explanation}

Let us think of the Fibonacci sequence not as a list of numbers, but as the sequence of coefficients of a power series.  
In other words, we define a generating function
\[
F(x) = F_0 + F_1x + F_2x^2 + F_3x^3 + \cdots,
\]
where each coefficient corresponds to a Fibonacci number.  

Since the Fibonacci sequence satisfies the recurrence relation
\[
F_n = F_{n-1} + F_{n-2},
\]
we can express this recurrence in terms of $F(x)$ itself.  
To do that, consider how the series looks when multiplied by $x$ and $x^2$:

\[
\left\{
\begin{aligned}
	F(x) &= F_0 + F_1x + F_2x^2 + F_3x^3 + \cdots, \\
	xF(x) &= F_0x + F_1x^2 + F_2x^3 + F_3x^4 + \cdots, \\
	x^2F(x) &= F_0x^2 + F_1x^3 + F_2x^4 + F_3x^5 + \cdots.
\end{aligned}
\right.
\]

Now, if we take $F(x) - xF(x) - x^2F(x)$, all the shifted terms cancel out due to the recurrence relation, leaving only the initial conditions:
\[
F(x) - xF(x) - x^2F(x) = F_0 + (F_1 - F_0)x.
\]
Assuming $F_0 = 0$ and $F_1 = 1$, we obtain
\[
F(x) = \frac{x}{1 - x - x^2}.
\]

The denominator here encodes the same recurrence that defines Fibonacci numbers.  
To understand the structure of $F(x)$, we factor the quadratic polynomial:
\[
1 - x - x^2 = (1 - \varphi x)(1 - \psi x),
\]
where 
\[
\varphi = \frac{1 + \sqrt{5}}{2}, \quad \psi = \frac{1 - \sqrt{5}}{2}.
\]

By the method of partial fractions, we can decompose $F(x)$ as
\[
F(x) = \frac{A}{1 - \varphi x} + \frac{B}{1 - \psi x}.
\]

Each term now has a familiar geometric series form:
\[
\frac{1}{1 - r x} = 1 + r x + r^2x^2 + r^3x^3 + \cdots,
\]
so we can directly read off the coefficients as powers of $\varphi$ and $\psi$.  

This is why we deliberately rewrite $F(x)$ in such a form:  
it allows us to transform an abstract recurrence relation into a closed analytic expression.  
Eventually, by equating coefficients of $x^n$, we recover the celebrated Binet formula:
\[
F_n = \frac{\varphi^n - \psi^n}{\sqrt{5}}.
\]

\subsubsection{Formal Derivation}

We start from the Fibonacci recurrence
\[
F_0 = 0, \qquad F_1 = 1, \qquad F_n = F_{n-1} + F_{n-2} \quad (n \ge 2).
\]

Defining the generating function
\[
F(x) = \sum_{n=0}^{\infty} F_n x^n.
\]
Then
\[
xF(x) = \sum_{n=0}^{\infty} F_n x^{n+1}, \qquad
x^2F(x) = \sum_{n=0}^{\infty} F_n x^{n+2}.
\]

Applying the recurrence relation
\[
F(x) - xF(x) - x^2F(x)
= F_0 + (F_1 - F_0)x + \sum_{n=2}^{\infty}(F_n - F_{n-1} - F_{n-2})x^n.
\]
Since $F_n - F_{n-1} - F_{n-2} = 0$ for all $n \ge 2$, we get
\[
F(x) - xF(x) - x^2F(x) = x.
\]
Hence,
\[
F(x) = \frac{x}{1 - x - x^2}.
\]

Factorization and substitution.
Let
\[
1 - x - x^2 = (1 - \varphi x)(1 - \psi x),
\]
where
\[
\varphi = \frac{1+\sqrt{5}}{2}, \qquad
\psi = \frac{1-\sqrt{5}}{2}.
\]

Then
\[
F(x) = \frac{x}{(1 - \varphi x)(1 - \psi x)}
= A\frac{x}{1 - \varphi x} + B\frac{x}{1 - \psi x}.
\]

Solving for constants.
Multiplying both sides by $(1 - \varphi x)(1 - \psi x)$:
\[
x = A(1 - \psi x) + B(1 - \varphi x)
= (A + B) - (\psi A + \varphi B)x.
\]
Matching coefficients gives
\[
A + B = 0, \qquad \varphi B + \psi A = -1.
\]
Solving:
\[
A = \frac{1}{\varphi - \psi}, \qquad B = -\frac{1}{\varphi - \psi}.
\]

Geometric series expansion
\[
\frac{1}{1 - r x} = \sum_{n=0}^{\infty} r^n x^n.
\]
Thus,
\[
F(x)
= \frac{1}{\varphi - \psi}
\left(
\frac{x}{1 - \varphi x} - \frac{x}{1 - \psi x}
\right)
= \frac{1}{\varphi - \psi}
\sum_{n=1}^{\infty} (\varphi^n - \psi^n)x^n.
\]

Extracting coefficients.
The coefficient of $x^n$ gives
\[
F_n = \frac{\varphi^n - \psi^n}{\varphi - \psi}
= \frac{1}{\sqrt{5}}
\left[
\left(\frac{1+\sqrt{5}}{2}\right)^n
- \left(\frac{1-\sqrt{5}}{2}\right)^n
\right].
\]

\[
\boxed{
	F_n = \frac{\varphi^n - \psi^n}{\sqrt{5}}
}
\]
This is the closed form known as \textbf{Binet's formula}.

\subsubsection{Maclaurin series of generating function}
	For a function \( f(x) \) that is infinitely differentiable at \( x=0 \),
its Maclaurin series is defined as
\[
f(x) = \sum_{n=0}^{\infty} \frac{f^{(n)}(0)}{n!} x^n.
\]
Hence, the coefficient of \(x^n\) in this expansion equals \( f^{(n)}(0)/n! \).

Let the Fibonacci sequence \(\{F_n\}\) satisfy
\[
F_0 = 0, \quad F_1 = 1, \quad F_{n} = F_{n-1} + F_{n-2}.
\]
Its generating function is
\[
F(x) = \sum_{n=0}^{\infty} F_n x^n.
\]
As we have shown earlier:
\[
F(x) = \frac{x}{1 - x - x^2}.
\]

Expanding \(F(x)\) into a Maclaurin series gives
\[
F(x) = \sum_{n=0}^{\infty} \frac{F^{(n)}(0)}{n!} x^n.
\]
Comparing this with
\[
F(x) = \sum_{n=0}^{\infty} F_n x^n,
\]
we conclude that
\[
F_n = \frac{F^{(n)}(0)}{n!}.
\]

For \( n = 5 \),
\[
F_5 = \frac{F^{(5)}(0)}{5!}.
\]

We can perform a symbolic experiment to verify this relationship.
\lstinputlisting[language=Python, caption={Symbolic verification in Python (SymPy)}]{./maclaurin-series/maclaurin-series.py}
Seems good, at least for first 20 numbers : \\
\begin{center}
\begin{tabular}{c|c|c}
	n & Maclaurin & Reference \\
	\hline
	1 & 1 & 1 \\
	2 & 1 & 1 \\
	3 & 2 & 2 \\
	4 & 3 & 3 \\
	5 & 5 & 5 \\
	6 & 8 & 8 \\
	7 & 13 & 13 \\
	8 & 21 & 21 \\
	9 & 34 & 34 \\
	10 & 55 & 55 \\
	11 & 89 & 89 \\
	12 & 144 & 144 \\
	13 & 233 & 233 \\
	14 & 377 & 377 \\
	15 & 610 & 610 \\
	16 & 987 & 987 \\
	17 & 1597 & 1597 \\
	18 & 2584 & 2584 \\
	19 & 4181 & 4181 \\
	20 & 6765 & 6765 \\
\end{tabular}
\end{center}

This calculation has no practical purpose whatsoever.  
It is a purely theoretical (and slightly overcomplicated) way to verify something we already know perfectly well.  
It was done entirely for fun and curiosity, just to see that the Maclaurin series machinery indeed reproduces Fibonacci numbers symbolically.

% Analyzing time complexity
\subsection{Time Complexity (Big $\mathcal{O}$)}
The recursive algorithm generates a binary recursion tree, where each node for \( n \geq 2 \) spawns two child nodes: \( F(n-1) \) and \( F(n-2) \).
The total number of function calls corresponds to the number of nodes in the recursion tree. For a given \( n \), the tree has a depth of approximately \( n \), and the number of nodes grows exponentially. The recurrence relation for the number of operations \( T(n) \) is:
\[
T(n) = T(n-1) + T(n-2) + O(1),
\]
where \( O(1) \) accounts for the constant-time addition operation. The base cases are:
\[
T(0) = O(1), \quad T(1) = O(1).
\]

This recurrence is similar to the Fibonacci sequence itself. The number of nodes is approximately \( 2F(n) - 1 \), where \( F(n) \approx \phi^n / \sqrt{5} \), and \( \phi = \frac{1 + \sqrt{5}}{2} \approx 1.618 \) is the golden ratio. Thus, the time complexity is:
\[
T(n) = O(\phi^n) \approx O(1.618^n).
\]

%%%%%%%%%%%%
\subsection{Empirical Validation of Time Complexity}
\subsubsection{Measuring execution time}
Let's calculate execution time of first 50 Fibonacci numbers. Also, save exec results in CSV file further for analysis.
\lstinputlisting[language=Java, caption={Measure execution time}]{./recursive-time-exec/MeasureExecTime.java}
To show how the time grows, let's build a chart. A ltiile bit Pandas and Matplotlib magic here.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{./recursive-time-exec/exponential-time-growth.png}
	\caption{Exponent execution time}
	\label{fig:exponent_growth}
\end{figure}

%To produce this image we use Python with some Pandas :
%\lstinputlisting[language=Python, caption={Exponential growth visualization}]{./recursive-time-exec/fib-exec-time-chart.py}
\subsubsection{Algorithmic fitting of time complexity}
Now we are interested in exact formula of this type of growth. To achieve our goal we'll going to use SciPy.
\lstinputlisting[language=Python, caption={Approximation of time complexity}]{./recursive-time-exec/empiristic-formula-for-time-complexity-cut.py}
In this Python script, we analyze the time complexity of an algorithm empirically by fitting an exponential model to measured execution times.


We assume that the running time $T(n)$ of the algorithm grows approximately exponentially with the input size $n$, according to the model

\[
	T(n) = a \, b^n,
\]

where $a > 0$ is a scaling constant and $b > 1$ determines the rate of growth.

The corresponding Python function defining this model is:

\begin{lstlisting}[language=Python]
def exponential_model(n, a, b):
    return a * b ** n
\end{lstlisting}

The script reads a CSV file named \texttt{fibonacci\_data.csv} that contains two columns:

\begin{itemize}
	\item \texttt{n} — the input size or recursion depth.
	\item \texttt{time\_sec} — the measured running time in seconds.
\end{itemize}

\begin{lstlisting}[language=Python]
data = pd.read_csv('fibonacci_data.csv')
n = data['n'].values
time_sec = data['time_sec'].values
\end{lstlisting}

The parameters $a$ and $b$ are estimated using the \texttt{curve\_fit} function from the \texttt{scipy.optimize} module. This function minimizes the squared error between the empirical data and the model prediction.

An initial guess for the parameters is provided as $a_0 = 10^{-6}$ and $b_0 = 1.618$

\begin{lstlisting}[language=Python]
popt, pcov = curve_fit(exponential_model, n, time_sec,
                       p0=[1e-6, 1.618])
a, b = popt
\end{lstlisting}

After fitting, the model parameters are printed in the form

\[
	T(n) = a \cdot b^n.
\]

Finally, the script computes the predicted execution times using the fitted parameters:

\begin{lstlisting}[language=Python]
predicted_time = exponential_model(n, a, b)
\end{lstlisting}

This allows comparing the theoretical exponential model with the observed data to verify whether the algorithm’s time complexity indeed follows an exponential trend.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{./recursive-time-exec/experimental-data-formula.png}
	\caption{Model fit to experimental data}
	\label{fig:experimental_vs_theoretical}
\end{figure}

Axes made logarithmic for better looking chart.
So, we have result of out numerical modeling 
\begin{verbatim}
python empiristic-formula-for-time-complexity.py
Fitted model: T(n) = 0.0000000027 * 1.600990^n
Golden ratio (φ): 1.618034
Deviation of b from φ: 0.017044
\end{verbatim}

%%%%%%%%%%%% CURVE FIT
The function \texttt{curve\_fit} from the \texttt{scipy.optimize} library estimates the parameters of a nonlinear model by minimizing the squared difference between the observed data and the model predictions.

Formally, for a given model function
\[
	f(n, \boldsymbol{\theta}) = f(n, a, b),
\]
and experimental data points $\{(n_i, y_i)\}$, the algorithm finds the parameter vector
\[
	\boldsymbol{\theta}^* = \arg\min_{\boldsymbol{\theta}} \sum_i \big( y_i - f(n_i, \boldsymbol{\theta}) \big)^2.
\]

This is a \emph{nonlinear least squares} problem.

\medskip
\noindent
Internally, \texttt{curve\_fit} uses the \textbf{Levenberg–Marquardt algorithm}, which combines the ideas of:
\begin{itemize}
	\item the \emph{Gauss–Newton method} — efficient for near-linear problems, and
	\item \emph{gradient descent} — stable for highly nonlinear models.
\end{itemize}

The algorithm iteratively adjusts the parameters $(a, b)$ to reduce the residual sum of squares. In each iteration, it:
\begin{enumerate}
	\item computes the current model values $f(n_i, a, b)$,
	\item evaluates the Jacobian matrix of partial derivatives,
	\item updates the parameters using a damping factor that balances between Gauss–Newton and gradient descent steps.
\end{enumerate}

\medskip
\noindent
If the residual error stops decreasing or becomes sufficiently small, the algorithm is considered to have \emph{converged}, and the best-fit parameters are returned.

\begin{lstlisting}[language=Python]
popt, pcov = curve_fit(exponential_model, n, time_sec)
\end{lstlisting}

Here, \texttt{popt} contains the estimated parameters $[a, b]$, while \texttt{pcov} is the covariance matrix that quantifies their uncertainty.

Full explanation on how it works is a bit out of disscussion scope here.
%%%%%%%%%%%%

We can have exec time needed to calculate n-th fib - $T(n) = 0.0000000027 * 1.600990^n$. It grows fast and could be very big.

\begin{table}[h]
	\centering
	\caption{Estimated execution time of recursive Fibonacci algorithm using the model $T(n) = 2.7 \times 10^{-9} \cdot 1.600990^n$}
	\begin{tabular}{|c|c|c|}
		\hline
		$n$ & $T(n)$ & Unit \\
		\hline
		20 & 3.30e-05 & sec \\ 
		\hline
		30 & 3.66e-03 & sec \\ 
		\hline
		40 & 4.04e-01 & sec \\ 
		\hline
		50 & 4.48e+01 & sec \\ 
		\hline
		60 & 1.3752 & hr \\ 
		\hline
		70 & 6.3395 & days \\ 
		\hline
		80 & 1.9215 & yr \\ 
		\hline
		90 & 212.5851 & yr \\ 
		\hline
		100 & 23519.0125 & yr \\ 
		\hline
		
	\end{tabular}
\end{table}

Now, we have kind of bad news here. To calculate $F(100)$ we'll need $\approx 25 000$ years. Too long to wait, soon we improve algorithm.

% Creating a section for JVM Memory Structures
\subsection{Introduction to JVM Memory Structures}

% Introducing the JVM and its memory architecture
The Java Virtual Machine (JVM) is a runtime environment that executes Java bytecode, enabling platform-independent execution of Java programs. The JVM manages memory through a structured architecture that supports dynamic allocation, thread execution, and garbage collection.

% Subsection for overview of JVM memory

The JVM divides its memory into several regions, each serving a specific purpose in program execution. These regions are broadly categorized into \textbf{per-thread} and \textbf{shared} areas:
\begin{itemize}
	\item \textbf{Per-Thread Areas}: Allocated for each thread to ensure isolation and manage method execution.
	\item \textbf{Shared Areas}: Accessible by all threads for storing objects and class metadata.
\end{itemize}
Memory management is critical for performance, as it affects allocation speed, garbage collection, and thread synchronization. The JVM's memory model is defined by the Java Virtual Machine Specification (JVMS).

The JVM's memory is organized into the following key areas:
\begin{enumerate}
	\item \textbf{Heap}:
	\begin{itemize}
		\item A shared memory region where all objects and arrays are allocated using the \texttt{new} keyword.
		\item Divided into:
		\begin{itemize}
			\item \textbf{Young Generation} (Eden and Survivor spaces): For newly created objects, managed by frequent minor garbage collections.
			\item \textbf{Old Generation}: For long-lived objects, managed by less frequent major garbage collections.
			\item \textbf{Metaspace} (Java 8+): Stores class metadata, replacing the Permanent Generation (pre-Java 8).
		\end{itemize}
		\item Configurable via flags like \texttt{-Xmx} (maximum heap size) and \texttt{-Xms} (initial heap size).
	\end{itemize}
	\item \textbf{Java Stack}:
	\begin{itemize}
		\item A per-thread memory area that stores \textbf{call frames} for method invocations.
		\item Each frame contains a local variable array, operand stack, and frame data (e.g., program counter, return address).
		\item Size is configurable via \texttt{-Xss}. Excessive recursion can cause a \texttt{StackOverflowError}.
	\end{itemize}
	\item \textbf{Program Counter (PC) Register}:
	\begin{itemize}
		\item A per-thread register that holds the address of the current bytecode instruction being executed.
		\item Points to the current instruction in the active call frame's bytecode.
	\end{itemize}
	\item \textbf{Method Area}:
	\begin{itemize}
		\item A shared area that stores class metadata, including bytecode, constant pools, and method tables.
		\item In Java 8+, the Method Area is implemented as Metaspace, which uses native memory rather than the heap.
	\end{itemize}
	\item \textbf{Native Method Stack}:
	\begin{itemize}
		\item A per-thread stack for executing native methods (e.g., C/C++ code called via JNI).
		\item Similar to the Java stack but tailored for non-Java code.
	\end{itemize}
\end{enumerate}

\subsection{Method Execution in Java}

\subsubsection{Core concepts}
Java’s method invocation and parameter passing mechanisms are central to understanding its runtime behavior in the Java Virtual Machine (JVM). Java exclusively uses \textbf{pass-by-value} for all parameter passing, impacting both iterative and recursive methods. Recursion, including \textbf{tail recursion}, interacts with the JVM’s stack and heap, while Java’s lack of \textbf{tail call optimization (TCO)} affects performance in deep recursion. 
\begin{itemize}
	\item \textbf{Pass-by-Value}: The method receives a copy of the argument’s value (primitive or object reference). Changes to the parameter do not affect the caller’s variable.
	\item \textbf{Pass-by-Reference}: The method receives a reference to the original argument’s memory location, so changes directly modify the caller’s variable.
\end{itemize}
Java uses \textbf{pass-by-value} exclusively. For \textbf{primitive types} (e.g., \texttt{int}, \texttt{double}), the value is copied. For \textbf{object references}, the reference (not the object) is copied, allowing modification of the object’s state in the heap but not reassignment of the caller’s reference.

% Explaining pass-by-value in Java
\subsubsection{Pass-by-Value in Java}
When a method is called, the JVM creates a call frame on the thread’s Java stack, copying arguments into the frame’s local variable array:
\begin{itemize}
	\item \textbf{Primitive Types}: The value (e.g., 5 for an \texttt{int}) is copied. Modifying the parameter changes only the local copy.
	\item \textbf{Object References}: The reference (memory address to a heap object) is copied. Modifying the object’s state (e.g., fields) affects the heap, visible to all references. Reassigning the reference (e.g., \texttt{obj = new Object()}) is local.
\end{itemize}
This behavior applies to both iterative and recursive methods, but recursion increases stack depth, risking \texttt{StackOverflowError} for deep calls.

To demonstrate pass-by-value, let’s use a \texttt{StringBuilder} object, which is mutable and allows us to modify its contents.
This example will show how pass-by-value works with object references and clarify the distinction between modifying an object’s state and reassigning a reference.

\lstinputlisting[language=Java,caption={Pass-by-Value Demonstration in Java, nolol}]{./jvm-memory-structures/PassByValueExample.java}

When we run this program, we get the following output:

\begin{verbatim}
Before method calls: number = 10, sb = Hello
After method calls: number = 10, sb = Hello, World!
\end{verbatim}
This output illustrates the pass-by-value behavior:
\begin{itemize}
	\item \texttt{Primitive Type (int)}: In the \texttt{changeNumber} method, the parameter \texttt{x} is a copy of the value of \texttt{number}. Changing \texttt{x} to 20 inside the method does not affect the original \texttt{number} variable, as only a copy of its value was passed. Thus, \texttt{number} remains 10.
	\item \texttt{StringBuilder (modifyStringBuilder)}: In the \texttt{modifyStringBuilder} method, the \texttt{builder} parameter is a copy of the reference to the original \texttt{StringBuilder} object. When we call \texttt{builder.append(", World!")}, we modify the state of the object that \texttt{builder} points to. Since the original \texttt{sb} variable also points to the same object, the change is reflected in \texttt{sb}.
	\item \texttt{StringBuilder (reassignStringBuilder)}: In the \texttt{reassignStringBuilder} method, the \texttt{builder} parameter is reassigned to a new \texttt{StringBuilder} object. This changes the copy of the reference to point to a new object, but it does not affect the original \texttt{sb} variable, which still points to the original \texttt{StringBuilder} object. Thus, the output remains unchanged after this method call.
\end{itemize}

% Explaining tail recursion and lack of TCO
\subsubsection{Tail Recursion}
A method is \textbf{tail-recursive} if the recursive call is the final operation, with no pending computations. In languages with \textbf{tail call optimization (TCO)}, the runtime reuses the current frame, avoiding stack growth. Java’s JVM does not support TCO, so each recursive call creates a new frame, copying arguments via pass-by-value.
It's because JVM prioritizes general-purpose execution and accurate stack traces for debugging over TCO.
\\
TCO support varies across languages, impacting recursion efficiency:
\begin{itemize}
	\item \textbf{Java}: No TCO; each call adds a frame, risking \texttt{StackOverflowError}.
	\item \textbf{Scala}: TCO for self-recursive calls with \texttt{@tailrec}, compiling to loops on the JVM.
	\item \textbf{Python}: No TCO; uses iteration or trampolining for deep recursion.
	\item \textbf{JavaScript}: Partial TCO (e.g., Safari supports it, V8 does not).
	\item \textbf{Haskell}: Full TCO with lazy evaluation, ideal for recursion-heavy code.
\end{itemize}
\begin{table}[h]
	\centering
	\begin{tabular}{|>{\raggedright\arraybackslash}m{2.5cm}|>{\raggedright\arraybackslash}m{3cm}|>{\raggedright\arraybackslash}m{3cm}|}
		\hline
		\textbf{Language} & \textbf{TCO Support} & \textbf{Workaround} \\
		\hline
		Java & None & Iteration \\
		Scala & Yes (\texttt{@tailrec}) & None needed \\
		Python & None & Iteration \\
		JavaScript & Partial & Iteration \\
		Haskell & Full & None needed \\
		\hline
	\end{tabular}
	\caption{Tail Call Optimization Across Languages}
\end{table}

%%%%%%%%%%%%%%%%%%%%%
%%\subsection{Depth and \lstinline[style=keyword]\{StackOverflow\}}
\subsection{Depth and \lstinline[basicstyle=\ttfamily\small]{StackOverflow}}
Recursive calls create stack frames in the JVM, which can lead to a \texttt{StackOverflowError}. We demonstrate this with a simple recursive program :
\lstinputlisting[language=Java,caption={Testing Recursion Depth}]{./recursion-tree/RecursionDepth.java}
Run with:
\begin{lstlisting}[language=bash,caption={Running RecursionDepth}, nolol]
	java -Xss1m RecursionDepth
\end{lstlisting}

It means that even without calculating something, we limited by the value of stack. God news is that it could be increased, but we have no clue how much we need.

%%%%%%%%%%%%%%%%%%%%%
\subsection{Recursion Tree}
%% todo
\lstinputlisting[language=Java,caption={Recursion StackTrace}]{./recursion-tree/StackTraceDepth.java}
We have to have compare depth calculation and number of frames, make conclusions.

Output : 
\begin{lstlisting}[language=bash]
> java StackTraceDepth
fib0(5) depth=1 frames=3
fib0(4) depth=2 frames=4
fib0(3) depth=3 frames=5
fib0(2) depth=4 frames=6
fib0(1) depth=5 frames=7
fib0(0) depth=5 frames=7
fib0(1) depth=4 frames=6
fib0(2) depth=3 frames=5
fib0(1) depth=4 frames=6
fib0(0) depth=4 frames=6
fib0(3) depth=2 frames=4
fib0(2) depth=3 frames=5
fib0(1) depth=4 frames=6
fib0(0) depth=4 frames=6
fib0(1) depth=3 frames=5
Result: 5
\end{lstlisting}

Let's make an improvement. We'll all ident to previous code according to depth
\lstinputlisting[language=Java,caption={Recursion indent}]{./recursion-tree/SimpleIdent.java}
\newpage
Output is better. BTW code is good for debugging any recursion.
\begin{lstlisting}[language=bash]
fib0(5) depth=0
  fib0(4) depth=1
    fib0(3) depth=2
      fib0(2) depth=3
        fib0(1) depth=4
        => fib0(1) = 1
        fib0(0) depth=4
        => fib0(0) = 0
      => fib0(2) = 1
      fib0(1) depth=3
      => fib0(1) = 1
    => fib0(3) = 2
    fib0(2) depth=2
      fib0(1) depth=3
      => fib0(1) = 1
      fib0(0) depth=3
      => fib0(0) = 0
    => fib0(2) = 1
  => fib0(4) = 3
  fib0(3) depth=1
    fib0(2) depth=2
      fib0(1) depth=3
      => fib0(1) = 1
      fib0(0) depth=3
      => fib0(0) = 0
    => fib0(2) = 1
    fib0(1) depth=2
    => fib0(1) = 1
  => fib0(3) = 2
=> fib0(5) = 5
Result: 5
\end{lstlisting}

We can do better. Let's buid a tree using dot syntax (blah-blah-blah)
\lstinputlisting[language=Java, caption={Recursion tree generator}]{./recursion-tree/RecursionGraphvizTree.java}

And here it is : 
\begin{figure}[H] % The figure environment helps with placement and captions
	\centering
	\includegraphics[width=0.8\textwidth]{./recursion-tree/fib\_tree.png}
	\caption{Recursion tree} % Caption for the image
	\label{fig:logo} % Label for cross-referencing
\end{figure}

\newpage
As you can see in Figure \ref{fig:logo}, recursion tree is displayed nicely.

Or even better
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{./recursion-tree-animation/png/recursion_tree_combined.png}
	\caption{Recursion progress}
	\label{fig:fib_steps_progress}
\end{figure}

% Analyzing space complexity
\subsection{Space Complexity}
The space complexity is determined by the memory used on the call stack due to recursion. Although the recursion tree contains an exponential number of nodes (\( O(\phi^n) \)), not all nodes are active simultaneously. The call stack only holds the frames for the active recursive calls along a single path from the root to a leaf.

\subsubsection{Call Stack Analysis}
Consider the recursion tree for computing \( F(n) \). The deepest path in the tree occurs when the recursion follows \( F(n) \to F(n-1) \to F(n-2) \to \cdots \to F(0) \), which has a depth of \( n \). At any point, the call stack contains at most \( n \) frames, each storing a constant amount of data (e.g., the parameter \( n \) and return address).

For example:
- When computing \( F(n-1) \), the call for \( F(n-2) \) is not yet active.
- Once \( F(n-1) \) is resolved, its stack frame is popped, and \( F(n-2) \) is pushed onto the stack.

Thus, the maximum stack depth is \( n \), leading to a space complexity of:
\[
O(n).
\]

\subsubsection{Clarification on Exponential Misconception}
The total number of recursive calls is exponential, which might suggest exponential memory usage. However, since only one path of the recursion tree is active at a time, the call stack grows linearly with \( n \), not exponentially.

\subsection{JVM Debugger view}
To show how recursion works in JVM, we can use Java Debug Interface (JDI) to create a simple debugger. Subject of our interest is call stack, so we will set breakpoint in Fibonacci method and step into it.
Here it is:
\lstinputlisting[language=Java, caption={Minimal Debugger}, linerange={public\ class\ MinimalDebugger-}]{./fibonacci-debugger/MinimalDebugger.java}
We are not reuse previous Fibonacci code, but create a new one for debugger target :
\lstinputlisting[language=Java, caption={Debugger Target}, nolol]{./fibonacci-debugger/FibonacciTarget.java}
It has main method, so we can set breakpoint there and step into Fibonacci method.
We should compile it with debugging info enabled :
\begin{lstlisting}[language=bash]
	javac -g -cp .;%JAVA_HOME%/lib/tools.jar *.java
\end{lstlisting}
To run app in debug mode we use this: 
\begin{lstlisting}[language=bash]
	java -cp .;%JAVA_HOME%/lib/tools.jar -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005 FibonacciTarget
\end{lstlisting}
It will wait for debugger to attach to port 5005. Execution staarts when debugger attaches.
Then run debugger : 
\begin{lstlisting}[language=bash]
	java -cp .;%JAVA_HOME%/lib/tools.jar MinimalDebugger
\end{lstlisting}

Exploring debugger output : 
\lstinputlisting[language=bash, caption={Call frames in Java Debugger}, nolol]{./fibonacci-debugger/debugger-output.txt}

Debugger class diagram shown here

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{./fibonacci-debugger/diagrams/class/DebuggerClass.png}
	\caption{Debugger class diagram}
	\label{fig:debugger_class_diagram}
\end{figure}


% Conclusion
\subsection{Conclusion}
The recursive Fibonacci algorithm has:
\begin{itemize}
	\item \textbf{Time Complexity}: \( O(\phi^n) \), where \( \phi \approx 1.618 \), due to the exponential number of nodes in the recursion tree.
	\item \textbf{Space Complexity}: \( O(n) \), due to the linear depth of the call stack, as only one path of the recursion tree is active at any time.
\end{itemize}

\section{Optimizing Recursion}
\subsection{Memoization}
\lstinputlisting[language=Java,caption={Memoized Fibonacci}]{./optimizing-recursion/Memoization.java}
When computing recursive functions such as the Fibonacci sequence, a large amount of redundant work is performed.
In a naive recursive implementation, many subproblems are solved multiple times.
For example, both calls to \texttt{fib(4)} and \texttt{fib(3)} appear several times in the recursion tree of \texttt{fib(5)}.
Recursion, by itself, does not remember previously computed results.

\textbf{Memoization} is a technique that introduces \emph{memory} into recursion.
The key idea is simple:

\begin{quote}
	If a function has already computed the value for some argument $n$,
	it should store that result and return it directly next time instead of recomputing it.
\end{quote}

This means that each distinct subproblem is solved only once,
and subsequent recursive calls simply look up the answer in a table (often called a \emph{memo}).

\begin{enumerate}
	\item The first time a function such as \texttt{fib(n)} is called, it performs the usual recursive computation.
	\item After computing a value, it is stored in a table (for example, a \texttt{HashMap} in Java) using $n$ as the key.
	\item If the same function is later called with the same argument, it first checks whether that key already exists in the table:
	\begin{itemize}
		\item If yes --- return the stored value immediately.
		\item If no --- compute it recursively and then store it.
	\end{itemize}
\end{enumerate}

This approach converts an exponential recursion (with repeated subtrees) into a linear process where each subproblem is evaluated only once.

A \texttt{HashMap} provides average constant-time ($O(1)$) access to stored values.
Each entry in the map corresponds to one subproblem:
\[
	\text{key} = n, \qquad \text{value} = F(n)
\]
Checking whether a result is already known and retrieving it from memory thus takes negligible time compared to performing another recursive expansion.

Recursion can be imagined as a tree of function calls that grows exponentially.
Memoization ``cuts off'' branches that lead to already known results,
turning the tree into a straight line of unique subproblems.

Memoization is not limited to Fibonacci numbers.
It applies to any recursive problem that:
\begin{itemize}
	\item exhibits \textbf{overlapping subproblems}, and
	\item depends only on its input arguments (i.e., is a \emph{pure function}).
\end{itemize}

In fact, this idea forms the theoretical foundation of \textbf{dynamic programming}:
\emph{store intermediate results to avoid recomputation}.

%% So, time complexity: $T(n) = \mathcal{O}(n)$ and space complexity: $M(n) = \mathcal{O}(n)$.

In the naive recursive Fibonacci function, the number of function calls grows approximately as $\varphi^n$.
This happens because each call branches into two further calls, leading to an exponential explosion of overlapping subproblems.

With memoization, however, every value of \texttt{fib(k)} for $k \le n$ is computed only once and then stored.
As a result, the total number of computations becomes linear in $n$:

\[
	T_{\text{naive}}(n) = O(\varphi^n)
	\qquad \Longrightarrow \qquad
	T_{\text{memoized}}(n) = O(n)
\]

Each access to the memoization table (implemented as a \texttt{HashMap}) takes constant expected time, $O(1)$,
so the overall asymptotic cost is dominated by the number of unique subproblems.

The memory usage also grows linearly, since one value is stored for each index:

\[
	S_{\text{memoized}}(n) = O(n)
\]

\subsection{Bottom-Up Dynamic Programming Approach}
After introducing memoization as a \emph{top-down} optimization,
we can take the same idea one step further by removing recursion entirely.
Instead of computing subproblems on demand, we build the table of results
\emph{iteratively} from the smallest cases upward.
This approach is known as \textbf{bottom-up dynamic programming}.

\lstinputlisting[language=Java,caption={Botton-up Fibonacci}]{./optimizing-recursion/BottomUpFibonacci.java}

This version eliminates recursion by directly constructing an array
where each entry \texttt{fib[i]} stores the $i$-th Fibonacci number.
The first two values are initialized to $F_0 = 0$ and $F_1 = 1$,
and then a single loop fills the rest of the array using the recurrence:
\[
	F_i = F_{i-1} + F_{i-2} \qquad \text{for } i \ge 2
\]
Since every previous value is already known, there is no need to perform any recursive calls.

Each Fibonacci number from $2$ to $n$ is computed exactly once,
so the total running time is linear:
\[
	T(n) = O(n)
\]
The algorithm also stores all intermediate results in an array of size $n + 1$,
resulting in a linear space complexity:
\[
	S(n) = O(n)
\]
Compared to memoization, this version avoids the recursive call stack and uses
a fixed amount of iterative control.
\\
Conceptual comparison :
\begin{itemize}
	\item \textbf{Memoization (top-down):} recursion + caching previously computed results.
	\item \textbf{Dynamic programming (bottom-up):} build the same table iteratively.
\end{itemize}

Both methods rely on the same principle:
\emph{each subproblem is solved only once}.
The bottom-up form, however, is usually more memory-efficient
and avoids the overhead of recursive function calls.


\subsection{Iterative Fibonacci and Space Optimization}

The bottom-up dynamic programming approach eliminates recursion,
but still stores all intermediate values in an array of size $O(n)$.
However, when computing the Fibonacci sequence, each new term depends only on the previous two values.
This observation allows us to remove the array entirely and keep only two variables that ``slide'' through the sequence.

\lstinputlisting[language=Java,caption={Space-Optimized DP Fibonacci}]{./optimizing-recursion/IterativeFibonacci.java}

Instead of storing the entire DP table, we can maintain only:
\[
	\texttt{prev} = F_{i-2}, \qquad \texttt{curr} = F_{i-1}
\]
and compute
\[
	\texttt{next} = \texttt{prev} + \texttt{curr}
\]
at each step, shifting the variables forward:
\[
	\texttt{prev} \leftarrow \texttt{curr}, \qquad
	\texttt{curr} \leftarrow \texttt{next}
\]

This simple loop reproduces the same results as the bottom-up method,
but with constant memory and no recursion at all.

Time and Space Complexity.

The number of loop iterations is linear in $n$,
so the time complexity remains:
\[
	T(n) = O(n)
\]
However, the space complexity improves dramatically:
\[
	S(n) = O(1)
\]
since only three variables (\texttt{prev}, \texttt{curr}, \texttt{next}) are needed, regardless of $n$.

\begin{center}
	\begin{tabular}{lccc}
		\toprule
		\textbf{Approach} & \textbf{Recursion} & \textbf{Extra Memory} & \textbf{Asymptotic Time} \\
		\midrule
		Naive recursion & Yes & None & $O(\varphi^n)$ \\
		Memoization (top-down DP) & Yes & $O(n)$ & $O(n)$ \\
		Bottom-up DP (array) & No & $O(n)$ & $O(n)$ \\
		Iterative (optimized DP) & No & $O(1)$ & $O(n)$ \\
		\bottomrule
	\end{tabular}
\end{center}

This version represents the final stage of optimization:
\begin{itemize}
	\item The recursion tree disappears completely.
	\item Memory use shrinks to a few constant variables.
	\item Each Fibonacci number is computed in one pass.
\end{itemize}

Conceptually, the computation becomes a simple linear flow:
\[
	F_0 \rightarrow F_1 \rightarrow F_2 \rightarrow \cdots \rightarrow F_n
\]
where each new term is built from the two preceding ones.
%% Time Complexity: $T(n) = \mathcal{O}(n)$ Space Complexity: $M(n) = \mathcal{O}(1)$ (optimized version)

\subsection{Dynamic Programming: Top-Down vs Bottom-Up}

Dynamic Programming (DP) is a general strategy for solving problems that can be decomposed into
\textbf{overlapping subproblems} and exhibit \textbf{optimal substructure}.
This means that:
\begin{itemize}
	\item the same subproblems occur multiple times during computation, and
	\item the optimal solution to the whole problem can be built from optimal solutions of its parts.
\end{itemize}

The Fibonacci sequence is a canonical example of such a problem.
There are two main ways to implement dynamic programming algorithms:
\textbf{top-down} (with memoization) and \textbf{bottom-up} (with iteration).
\paragraph{Top-Down (Memoization)}
\begin{itemize}
	\item Start with the original problem (e.g., $F(n)$) and solve it recursively.
	\item Each recursive call stores its result in a table (a \texttt{Map} or an array).
	\item If a value has already been computed, it is reused instead of recomputed.
\end{itemize}
This approach mirrors the recursive structure of the mathematical definition and is often easier to write and reason about.
However, it relies on recursion and may cause stack overflow for very deep recursion depths.

\paragraph{Bottom-Up (Tabulation)}
\begin{itemize}
	\item Identify the smallest subproblems (the \emph{base cases}).
	\item Build a table iteratively, filling it from smaller indices to larger ones.
	\item Each new value is computed from already known previous values.
\end{itemize}
This approach avoids recursion completely.
It usually requires more careful setup but is more memory- and time-efficient.

\begin{center}
	\begin{tabular}{lll}
		\toprule
		& \textbf{Top-Down (Memoization)} & \textbf{Bottom-Up (Tabulation)} \\
		\midrule
		Computation order & On-demand (recursive) & Sequential (iterative) \\
		Storage structure & HashMap / array & Array / table \\
		Call mechanism & Recursion + cache & Loop \\
		Execution flow & From problem to subproblems & From subproblems to problem \\
		Memory use & $O(n)$ & $O(n)$ or less (can be $O(1)$) \\
		Best for & Conceptual clarity & Performance \\
		\bottomrule
	\end{tabular}
\end{center}

Both strategies embody the same principle:
\begin{quote}
	\emph{Do not recompute the same subproblem twice.}
\end{quote}

Memoization can be seen as a \textbf{lazy} version of dynamic programming
(solve subproblems only when needed),
while bottom-up tabulation is an \textbf{eager} version
(precompute all subproblems in advance).

In real-world applications, the choice between top-down and bottom-up depends on:
\begin{itemize}
	\item \textbf{Problem structure:} recursive vs iterative dependencies.
	\item \textbf{Available memory and stack depth.}
	\item \textbf{Ease of implementation:} top-down code is often more natural for tree-like problems,
	while bottom-up suits array-like or sequential problems.
\end{itemize}
It's all for now about dynamic programming. Let's move to another interesting approach. We'll do some math and improve our Fibonacci implementation even more.
From $\mathcal{O}(n)$ to $\mathcal{O}(\log n)$.

\subsection{Linear algebra and Fibonacci}
\subsubsection{Matrices and Transformations}
We begin with the general concepts of matrices as linear transformations and matrix multiplication, then apply these ideas to the Fibonacci sequence, deriving its matrix form and geometric meaning.

Matrices represent linear transformations in vector spaces. A linear transformation \( T: \mathbb{R}^m \to \mathbb{R}^n \) preserves vector addition and scalar multiplication:
\[
T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v}), \quad T(c \mathbf{u}) = c T(\mathbf{u}).
\]
For example, a 2x2 matrix defines a transformation \( T: \mathbb{R}^2 \to \mathbb{R}^2 \). Geometrically, such transformations can include rotations, scalings, shears, or reflections in the plane.

In the context of sequences like Fibonacci, we will use a specific matrix to model the recurrence as a linear transformation that iteratively evolves a state vector.

Matrix multiplication is defined such that the product \( AB \) represents the composition of the linear transformations corresponding to \( B \) followed by \( A \). For two matrices \( A \) (an \( m \times n \) matrix) and \( B \) (an \( n \times p \) matrix), the element \( (AB)_{ij} \) is the dot product of the \( i \)-th row of \( A \) and the \( j \)-th column of \( B \):
\[
(AB)_{ij} = \sum_{k=1}^n a_{ik} b_{kj}.
\]
This definition arises from the need to compose linear transformations. If \( T_A \) is the transformation for \( A \) and \( T_B \) for \( B \), then \( T_A(T_B(\mathbf{v})) = (AB) \mathbf{v} \).
Improve this def !!!

Geometrically, matrix multiplication corresponds to applying one transformation after another. For example, multiplying rotation matrices composes rotations; scaling followed by shearing distorts shapes accordingly. In sequences, repeated multiplication evolves the state over multiple steps, leading to growth or convergence patterns observable in geometric structures.

Building on linear transformations, we express the Fibonacci recurrence in matrix form. Represent a pair of consecutive Fibonacci numbers as a vector:
\[
\begin{bmatrix} F_n \\ F_{n-1} \end{bmatrix}.
\]
We seek a matrix \( A = \begin{bmatrix} a & b \\ c & d \end{bmatrix} \) such that:
\[
A \cdot \begin{bmatrix} F_n \\ F_{n-1} \end{bmatrix} = \begin{bmatrix} F_{n+1} \\ F_n \end{bmatrix}.
\]
Multiplying matrix to vector gives us the system of equations:
\begin{equation}
	\left\{
	\begin{aligned}
		a F_n + b F_{n-1} &= F_{n+1}\\
		c F_n + d F_{n-1} &= F_n
	\end{aligned}
	\right.
	\label{sys_fib}
\end{equation}

From the Fibonacci recurrence, we know:
\[
F_{n+1} = F_n + F_{n-1}.
\]
Comparing this with the first equation from system (\ref{sys_fib}), we get:
\[
a F_n + b F_{n-1} = F_n + F_{n-1}.
\]
For this to hold for all \( n \), the coefficients must match:
\[
a = 1, \quad b = 1.
\]
For second equation from (\ref{sys_fib}), we need:
\[
c F_n + d F_{n-1} = F_n.
\]
This implies:
\[
c = 1, \quad d = 0.
\]
Thus, the matrix is:
\[
A = \begin{bmatrix} 1 & 1 \\ 1 & 0 \end{bmatrix}.
\]
\\
This matrix acts as a linear transformation:
\[
A \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} x + y \\ x \end{bmatrix},
\]
which is a shear transformation. 

To find later terms, we use matrix powers:
\begin{equation}
	\begin{bmatrix} F_{n+1} \\ F_n \end{bmatrix} = \begin{bmatrix} 1 & 1 \\ 1 & 0 \end{bmatrix}^n \cdot \begin{bmatrix} 1 \\ 0 \end{bmatrix}
	\label{matrix_fib}
\end{equation}
This is what we were looking for - recurrence relation in matrix form for Fibonacci numbers. We'll visualize (\ref{matrix_fib}) later because it's fun.

\subsubsection{Algorithm implementation in Java}
\lstinputlisting[language=Java,caption={Fibonacci by Matrix exponentiation}]{./optimizing-recursion/MatrixFibonacci.java}

\subsubsection{Time and space complexity}
Time Complexity: $T(n) = \mathcal{O}(\log n)$ \\
Space Complexity: $M(n) = \mathcal{O}(1)$

\subsection{Computer Algebra Systems}
Using Maple, we can compute Fibonacci numbers efficiently:
\lstinputlisting[language=Maple,caption={Fibonacci in Maple}]{./optimizing-recursion/fibonacci.mpl}
Run with:
\begin{lstlisting}[language=bash]
cmaple -q fibonacci.mpl >out.log
\end{lstlisting}

\subsection{Binet formula in real life}

% Introducing the Binet formula and its theoretical complexity
The Binet formula provides a closed-form expression for the \(n\)-th Fibonacci number:
\[
F(n) = \frac{\phi^n - \psi^n}{\sqrt{5}},
\]
where \(\phi = \frac{1 + \sqrt{5}}{2}\) (the golden ratio) and \(\psi = \frac{1 - \sqrt{5}}{2}\). Since it directly computes \(F(n)\) without iterative steps, it has a theoretical time complexity of \(O(1)\) for a single computation, assuming arithmetic operations (like exponentiation) are constant-time. However, despite this apparent efficiency, the Binet formula is impractical for large \(n\) in computational practice for several reasons.

% Listing reasons for impracticality
\begin{enumerate}
	\item \textbf{Numerical Instability with Floating-Point Arithmetic}:
	For large \(n\), \(\phi^n\) grows exponentially, while \(\psi^n\) (with \(|\psi| < 1\)) becomes extremely small. Their difference, \(\phi^n - \psi^n\), involves subtracting two nearly equal values, leading to significant round-off errors in floating-point arithmetic (e.g., \texttt{double} in Java or C++). This causes inaccuracies, as seen when computing \(F(71)\), where floating-point results may deviate (e.g., 308061521170131 instead of the correct 308061521170129).
	
	\item \textbf{High Precision Requirements with Arbitrary-Precision Arithmetic}:
	To mitigate floating-point issues, arbitrary-precision arithmetic (e.g., \texttt{BigDecimal} in Java) can be used. However, computing \(\phi^n\), \(\psi^n\), and \(\sqrt{5}\) requires high precision (e.g., hundreds of decimal places for \(n \approx 100\)) to ensure the result is an exact integer after division by \(\sqrt{5}\). This significantly increases computational cost, as each operation (exponentiation, division) scales with the number of digits, making the effective time complexity much worse than \(O(1)\).
	
	\item \textbf{Computational Overhead of Irrational Numbers}:
	The Binet formula involves irrational numbers (\(\phi\), \(\psi\), \(\sqrt{5}\)), which are computationally expensive to handle with high precision. In contrast, iterative methods using integer arithmetic (e.g., with \texttt{BigInteger}) involve only simple additions, which are faster and inherently exact for Fibonacci numbers, as they are integers by definition.
	
	\item \textbf{Scalability Issues for Large \(n\)}:
	For very large \(n\) (e.g., \(n = 1000\)), the precision required for \(\sqrt{5}\) and \(\phi^n\) grows proportionally to \(n\), as the number of digits in \(F(n)\) is approximately \(n \cdot \log_{10}(\phi)\). This makes Binet's formula slower than iterative or matrix-based methods, which have logarithmic complexity (\(O(\log n)\) with fast exponentiation) but operate on integers.
	
	\item \textbf{Implementation Complexity}:
	Implementing Binet's formula requires careful management of precision and rounding to ensure integer results, which adds complexity compared to the straightforward iterative approach (e.g., \(F(n) = F(n-1) + F(n-2)\)). The latter is simpler to code and debug, as it avoids dealing with floating-point or arbitrary-precision libraries.
\end{enumerate}

% Concluding with a comparison to iterative methods
In conclusion, while the Binet formula is theoretically \(O(1)\), its practical performance is hindered by numerical instability, high precision requirements, and computational overhead for irrational numbers. Iterative methods, or matrix exponentiation methods with \(O(\log n)\) complexity, are preferred in practice due to their simplicity, exactness, and efficiency, especially for large \(n\). For example, computing \(F(71)\) iteratively with \texttt{BigInteger} is faster and guarantees the correct result (308061521170129) without precision management.


\subsection{Conclusion}
We explored Fibonacci computation methods, from naive recursion ($\mathcal{O}(2^n)$) to matrix exponentiation ($\mathcal{O}(\log n)$). Below is a comparison:

\begin{table}[h]
	\centering
	\begin{tabular}{|l|c|c|}
		\hline
		\textbf{Method} & \textbf{Time Complexity} & \textbf{Space Complexity} \\
		\hline
		Naive Recursion & $\mathcal{O}(2^n)$ & $\mathcal{O}(n)$ \\
		Memoization & $\mathcal{O}(n)$ & $\mathcal{O}(n)$ \\
		Dynamic Programming & $\mathcal{O}(n)$ & $\mathcal{O}(1)$ \\
		Binet's Formula & $\mathcal{O}(1)$ & $\mathcal{O}(1)$ \\
		Matrix Exponentiation & $\mathcal{O}(\log n)$ & $\mathcal{O}(1)$ \\
		\hline
	\end{tabular}
	\caption{Complexity Comparison of Fibonacci Methods}
\end{table}

For large $n$, matrix exponentiation or BigDecimal-based DP are recommended.

\section{Limitations of Primitive Types for Large Fibonacci Numbers}
\subsection{Introduction}
When computing large Fibonacci numbers in Java, primitive data types---int, long, float, and double---encounter significant limitations due to their fixed memory allocation and design. These constraints manifest as issues with range, precision, and overflow behavior, which become critical as Fibonacci numbers grow exponentially. Specifically, \texttt{int} and \texttt{long} face overflow when numbers exceed their maximum values, while \texttt{float} and \texttt{double} suffer from loss of precision for large integers, resulting in inaccurate results. Understanding these limitations is crucial for selecting appropriate data types or alternative approaches, such as \texttt{BigInteger}, to handle large Fibonacci numbers effectively.

\subsection{Handling Large Numbers with BigInteger}
Unlike primitive types, \texttt{BigInteger} can represent numbers of any size, limited only by available memory, making it ideal for calculations requiring exact results, such as in cryptography or large-scale computations. It works by dynamically allocating memory to store digits and performing operations like addition and multiplication through software algorithms, rather than hardware instructions. However, this flexibility comes at the cost of slower performance and higher memory usage compared to primitives.
Here is an example of iterative algorithm which uses \texttt{BigInteger}
We'll use this code later to show what exactly precision loss and overflow for primitive data types. This code is good enough for calculating big numbers, so we would use it as a reference.
\lstinputlisting[language=Java,caption={BigInteger Fibonacci}]{./data-types-limitations/BigIntegerFibonacci.java}
Time Complexity: $T(n) = \mathcal{O}(n \cdot M)$, where $M$ is the cost of BigInteger operations. \\
Space Complexity: $M(n) = \mathcal{O}(1)$

\subsection{When double goes wild}
\subsubsection{$0.1 + 0.2 \ne 0.3$}
In Java, when running the program
\lstinputlisting[language=Java,caption={Unexpected sum jn Java}, nolol]{./data-types-limitations/DoubleExample.java}
the output is
\begin{verbatim}
	0.30000000000000004
\end{verbatim}
instead of exactly \texttt{0.3}. \\

If you think Java is broken, try it in JavaScript or Python\\
JavaScript:
\lstinputlisting[language=JavaScript,caption={Unexpected sum in JavaScript}, nolol]{./data-types-limitations/DoubleExample.js}
Python:
\lstinputlisting[language=Python,caption={Unexpected sum in Python}, nolol]{./data-types-limitations/DoubleExample.py}
Output :
\begin{lstlisting}[language=bash]
	>java DoubleExample
	0.30000000000000004
	
	>node DoubleExample.js
	0.30000000000000004
	
	>python a.py
	0.30000000000000004	
\end{lstlisting}

This behavior is not a bug, but a direct consequence of the \emph{IEEE 754 double-precision floating-point format}.

\subsubsection{IEEE 754 Representation}
A Java \texttt{double} uses 64 bits:
\begin{itemize}
	\item 1 bit for the \emph{sign},
	\item 11 bits for the \emph{exponent},
	\item 52 bits for the \emph{mantissa} (fraction).
\end{itemize}

The value is computed as
\[
(-1)^{\text{sign}} \times 1.\text{mantissa} \times 2^{(\text{exponent} - 1023)}.
\]

\subsubsection{Calculate as machines}
The decimal fraction $0.1 = \tfrac{1}{10}$ has no finite binary expansion.
In base 2 it becomes
\[
0.1_{10} = 0.00011001100110011\ldots_2
\]
with repeating pattern \texttt{0011}. Since only 52 bits are available for
the mantissa, this value is stored approximately as
\[
0.10000000000000000555\ldots
\]

Similarly,
\[
0.2 \approx 0.2000000000000000111, \quad
0.3 \approx 0.2999999999999999889.
\]
Therefore,
\[
0.1 + 0.2 \approx 0.3000000000000000444,
\]
which is printed as \texttt{0.30000000000000004}.
\\
The decimal system can exactly represent fractions like $1/10$ or $1/5$,
but binary cannot. Binary floating-point can exactly represent only fractions
whose denominators are powers of two (e.g., $1/2$, $1/4$, $1/8$). Numbers like
$1/10$ or $1/5$ become infinite binary fractions, which must be rounded.

\subsubsection{Conclusion}
The result \texttt{0.30000000000000004} is not an error, but the inevitable
effect of representing decimal fractions in binary with limited precision.
Understanding IEEE 754 is essential for writing correct numerical programs.

\subsection{Showcase : from int to double}
Now we'll try to calculate some Fibonacci numbers using primitive data types in Java and see their limitations. Going to use iterative approach for simplicity.
\lstinputlisting[
	language=Java,
	linerange={//LABEL:findLargestExactFibonacci-//LABEL:findLargestExactFibonacciEnd},
	includerangemarker=false,
	nolol
]{./data-types-limitations/LargestExactFibonacci.java}
As you can see we are trying to get largest Fibonacci number that fits in given data type. We have different strategies for different types.
When we add numbers, we have to check for overflow. For Integer we have special method \texttt{Math.addExact(a, b)} that throws \texttt{ArithmeticException} on overflow.
\lstinputlisting[
	language=Java,
	linerange={//LABEL:LongAdditionStrategy-//LABEL:LongAdditionStrategyEnd},
	includerangemarker=false,
	nolol
]{./data-types-limitations/LargestExactFibonacci.java}
For double and float we have different situation - it becomes Infinity or NaN at some point, because of precision loss. We'll perfom this check and rethrow an exception when it happens.
\lstinputlisting[
	language=Java,
	linerange={//LABEL:DoubleAdditionStrategy-//LABEL:DoubleAdditionStrategyEnd},
	includerangemarker=false,
	nolol
]{./data-types-limitations/LargestExactFibonacci.java}
And putting it all together :
\lstinputlisting[
	language=Java,
	linerange={//LABEL:LargestExactFibonacci-//LABEL:LargestExactFibonacciEnd},
	includerangemarker=false,
	nolol
]{./data-types-limitations/LargestExactFibonacci.java}
Result of calculation is presented below :
\lstinputlisting[
	language=Java,
	linerange={//LABEL:FibonacciResult-//LABEL:FibonacciResultEnd},
	includerangemarker=false,
	nolol
]{./data-types-limitations/LargestExactFibonacci.java}
Interesting part here is how we output floating point numbers. We use \texttt{BigDecimal} to represent them exactly and print without scientific notation.
Outpput is a bit surprising. For Integer and Long everything is fine, but for Float and Double we have some unexpected results.
\begin{verbatim}
Integer Fibonacci (46): F(46) = 1836311903
Long Fibonacci (92): F(92) = 7540113804746346429
Float Fibonacci (186): F(186) = 332825031969915840000000000000000000000
Double Fibonacci (1476): F(1476) = 130698922376339870000000000000000000... // ton of zeros
\end{verbatim}

% Creating a table for Java data types and their limitations
Have a looak at this table summarizing limitations of Java data types for Fibonacci calculations:
\begin{table}[H]
\centering
\small
\caption{Limits of Java Data Types for Fibonacci Calculations}
\begin{tabularx}{\textwidth}{l c X X}
\toprule
\textbf{Data Type} & \textbf{Size (bits)} & \textbf{Maximum Value} & \textbf{Limitations for Large Numbers} \\
\midrule
int & 32 & $2,147,483,647$ ($\approx 2.1 \times 10^9$) & Overflow at F(47) ($\approx 2.97 \times 10^9$) \\
long & 64 & $9,223,372,036,854,775,807$ ($\approx 9.2 \times 10^{18}$) & Overflow at F(93) ($\approx 1.22 \times 10^{19}$) \\
float & 32 & $\approx 3.4 \times 10^{38}$ & Loss of precision after $\approx 10^7$; inaccurate for F(186) \\
double & 64 & $\approx 1.8 \times 10^{308}$ & Loss of precision after $\approx 10^{15}$; inaccurate for F(1476) \\
\bottomrule
\end{tabularx}
\end{table}
More than that, we can calculate differences between BigInteger result and primitive types results to see how much precision we lost.
% Table for differences between primitive types and BigInteger
\begin{table}[H]
\centering
\small
\caption{Differences Between Primitive Type Fibonacci Numbers and BigInteger}
\begin{tabularx}{\textwidth}{c X X X X}
\toprule
\textbf{Index} & \textbf{int Difference} & \textbf{long Difference} & \textbf{float Difference} & \textbf{double Difference} \\
\midrule
1 & 0 & 0 & 0 & 0 \\
10 & 0 & 0 & 0 & 0 \\
20 & 0 & 0 & 0 & 0 \\
30 & 0 & 0 & 0 & 0 \\
46 & 0 & 0 & 223 & 0 \\
50 & Overflow & 0 & 1377 & 0 \\
92 & Overflow & 0 & $2.8106 \times 10^{12}$ & 429 \\
100 & Overflow & Overflow & $1.3046 \times 10^{14}$ & $8.4925 \times 10^{4}$ \\
186 & Overflow & Overflow & $7.8117 \times 10^{31}$ & $3.7679 \times 10^{22}$ \\
\bottomrule
\end{tabularx}
\end{table}

So, we can conclude that primitive data types in Java have significant limitations when calculating large Fibonacci numbers, leading to overflow and precision loss.
For accurate computations, especially for large indices, using \texttt{BigInteger} is essential.
%\section{Real-World Applications}
%\subsection{Fibonacci heaps for Dijkstra's algorithm optimization}
%bla-bla-bla
%\subsection{Fibonacci retracement levels in stock market analysis}
%bla-bla-bla
%\subsection{Some of pseudorandom number generators}
%bla-bla-bla


\begin{thebibliography}{9}
\bibitem{knuth}
Knuth, D. E. (1997). \textit{The Art of Computer Programming, Volume 1: Fundamental Algorithms}. Addison-Wesley.
\bibitem{maple}
MapleSoft. (2025). Fibonacci Function Documentation. \url{https://www.maplesoft.com/support/help/Maple/view.aspx?path=combinat/fibonacci}.
\bibitem{wiki}
Wikipedia. (2025). Fibonacci Number. \url{https://en.wikipedia.org/wiki/Fibonacci_number}.
\bibitem{curve_fit}
SciPy docs \url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html}
\end{thebibliography}

\end{document}
